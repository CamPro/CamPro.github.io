<p>[English Below] [Idea]</p><p> MẠNG XÃ HỘI VÀ INTERNET: MỞ RỘNG GÓC NHÌN HAY THÚC ĐẨY SỰ CỰC ĐOAN?</p><p> Chúng ta thường nhắc đến sự xuất hiện của internet và mạng xã hội như một phương thức để con người có thể mở rộng góc nhìn và ngày càng trở nên cởi mở hơn. Như Facebook kỳ vọng giúp mọi người trên thế giới có thể kết nối với nhau, hay Google đảm bảo con người tìm kiếm được gần như mọi thứ họ muốn.</p><p> Nhưng có thực sự là thế không? Hay rốt cục sự phát triển của Facebook chỉ giúp những kẻ cực đoan giống nhau tìm được nhau và ngày càng trở nên cực đoan hơn, và sự ưu việt của Google chỉ giúp những kẻ tin vào thuyết âm mưu càng tin tưởng hơn vào những thứ vô căn cứ vì có thể tìm thấy mọi bài đăng củng cố niềm tin của mình?</p><p> Vậy, câu hỏi của ngày hôm nay là, liệu ở nơi mọi người có thể tìm đến nhau như internet, chúng ta có thực sự giao lưu với nhau một cách hiệu quả và dần cởi mở, hay cũng chỉ như một buổi tiệc lớn, nơi các nhóm người thu mình lại vào một góc nhỏ và thậm chí còn trở nên cực đoan hơn cả ngoài đời?</p><p> 1. Chúng ta vốn luôn tự thu mình vào một góc.</p><p> Năm 2010, nhà hoạt động công nghệ và phương tiện truyền thông Eli Pariser đưa ra khái niệm về “bong bóng lọc” (filter bubble) mô tả cách thuật toán của các nền tảng lớn ảnh hưởng đến cách ta tiếp nhận thế giới. Theo Eli, tin tức hoặc thông tin người dùng tìm thấy trên các công cụ tìm kiếm hoặc mạng xã hội được lựa chọn tự động bởi thuật toán thông qua dữ liệu người dùng tự nguyện cung cấp khi sử dụng các công cụ/nền tảng này. </p><p> Dựa trên hành vi sử dụng trong quá khứ, ví dụ từ khóa tìm kiếm hoặc các thao tác tương tác (bỏ qua, dừng lại, click vào…), thuật toán sẽ sàng lọc và trả về cho người dùng những kết quả phù hợp. Trong video TED Talk trình bày về vấn đề này [1], Eli đã nêu ví dụ về kết quả tìm kiếm từ khóa “Ai Cập” trên Google của 2 người bạn - cho thấy một người nhận được tin tức về các vấn đề chính trị như khủng hoảng hoặc biểu tình; trong khi người kia nhận được các thông tin về du lịch ở trang đầu tiên. </p><p> Sự khác biệt trên được Eli gọi là “nội dung được cá nhân hóa”, có thể tạo ra tình trạng người dùng bị cô lập trong những thông tin hoặc quan điểm chính trị mà họ tán thành; trong khi không được tiếp cận hoặc không nhận ra sự tồn tại của những luồng quan điểm trái ngược hoặc khác biệt. Nói ngắn gọn, Eli lo ngại rằng các thuật toán tự động có khả năng khiến chúng ta mắc kẹt trong chiếc bong bóng thông tin.</p><p> Điều này giống với cảm giác của chúng ta khi luôn nghĩ rằng bản thân đã được tiếp cận với nhiều luồng quan điểm trái chiều trên Facebook, cho đến khi vô tình lướt newfeed Facebook bằng tài khoản của người lạ mới bất ngờ như thể mình vừa được tiếp cận một thế giới mới, cảm giác như đó là thế giới ngầm trước nay tưởng chừng như chưa từng tồn tại. Và lạ hơn nữa là thế giới ấy dường như rất đông, chỉ mỗi mình bạn không biết tới.</p><p> Nhưng thực sự mọi thứ có đơn giản đến thế?</p><p> Để trả lời câu hỏi này, hãy bắt đầu bằng việc xem thử người dùng tìm kiếm thông tin qua những phương tiện nào. Theo số liệu khảo sát được tiến hành trên khoảng 75,000 người ở 38 quốc gia trên cả 5 châu lục vào năm 2019 do Digital News Report cung cấp [2], không ít người dựa vào mạng xã hội để cập nhật tin tức. Không hề lạ khi loại hình truyền thông này đang dần thay chỗ những TV, báo giấy hay radio truyền thống; và cũng không lạ khi Facebook là nền tảng được truy cập nhiều nhất, với tỷ lệ chiếm đến 36%. Cùng với WhatsApp, Instagram và Twitter, tổng cộng có đến 84% người dùng xem các nền tảng này là nguồn cung cấp tin tức của họ. Một con số đáng suy nghĩ khi kết hợp với khái niệm “bong bóng lọc” của Eli.</p><p> Kế đến, sự tồn tại của các thuật toán cá nhân hóa nội dung là điều không thể phủ nhận. Thực ra đây là một cơ chế cần thiết để tối ưu hóa tổng thời lượng mà người dùng tương tác với nền tảng. Điều này đặc biệt quan trọng đối với các nền tảng thương mại trực tuyến, khi mà tính năng gợi ý sản phẩm của Amazon đã giúp gia tăng giá trị trung bình mỗi đơn hàng lên 50%, mang về cho gã khổng lồ ngành thương mại điện tử này mức tăng trưởng doanh thu đáng mơ ước 300% [3]. </p><p> Tuy nhiên, không phải mọi sự cá nhân hóa đều được thực hiện tự động bởi các thuật toán.</p><p> Bên cạnh việc cá nhân hóa được chọn trước (pre-selected personalisation), tồn tại một hình thức khác nữa là việc cá nhân hóa tự chọn (self-selected personalisation) [4] do chính người dùng tự nguyện thực hiện. Điều này đặc biệt quan trọng khi nói về việc tiếp nhận thông tin và tin tức. </p><p> Trước khi xuất hiện mạng xã hội, chính chúng ta đã tự mình đưa ra những lựa chọn như click hay bỏ qua một đường link, mua báo của toà soạn nào và lật đến trang bao nhiêu, hoặc chuyển kênh TV. Trên mạng xã hội, cũng tự chúng ta ẩn đi, bỏ like những thứ không thích, và góp phần chia sẻ những thứ ta thích hay đồng ý, như trước nay vẫn vậy. Nhưng khi quyền tự quyết trong lựa chọn thích hoặc không thích, kết hợp với thuật toán hiển thị của các nền tảng mạng xã hội, liệu người dùng có bị giới hạn trong một số lượng những nguồn tin hạn chế hay không?</p><p> Câu trả lời là không. </p><p> Khi so sánh hai nhóm người dùng, một nhóm dùng mạng xã hội để thu thập tin tức (cả có chủ đích lẫn không chủ đích) với những người hoàn toàn không dùng mạng xã hội, nghiên cứu [5] nhận thấy những người có dùng mạng xã hội nói chung, đặc biệt là nhóm dùng mạng xã hội một cách vô tư, hóa ra lại vô tình được tiếp xúc với nhiều nguồn tin hơn hẳn so với những người không dùng.</p><p> Trong trường hợp của các công cụ tìm kiếm, bản thân người dùng Google chắc chắn có sự chủ ý từ đầu hơn so với việc lướt Facebook. Lúc này, kết quả hiển thị được cá nhân hóa hai lần: thuật toán lọc ra những kết quả phù hợp và chính ta chọn đọc những cái mình chú ý. Vậy, khi có sự can thiệp của thuật toán, liệu “bong bóng lọc” có xuất hiện?</p><p> Kết quả, vẫn là không. Dựa trên dữ liệu khảo sát [6] từ người dùng tại 4 quốc gia là Anh, Mỹ, Đức và Tây Ban Nha, kết quả cho thấy người dùng công cụ tìm kiếm để tìm tin tức (1) trung bình dùng nhiều nguồn tin trực tuyến hơn, (2) có nhiều xu hướng tham khảo nguồn tin từ cả phe cánh tả và cánh hữu hơn và (3) có vốn tin tức cân bằng hơn do dùng cả nguồn tả khuynh lẫn hữu khuynh.</p><p> Và nhiều nghiên cứu khác gần đây cũng cho thấy kết quả tương tự. Ví dụ, khi so sánh kết quả tìm kiếm của nhiều người khác nhau; đặc biệt là người theo đảng Dân chủ và Cộng hòa tại Mỹ, sự khác biệt gần như không tồn tại [7]. Ngoài ra, khi so sánh với người đọc tin tức cố định (thường xuyên ghé thăm trang nào đó), người dùng mạng xã hội lại được tiếp xúc với số lượng nguồn tin đa dạng hơn hẳn [8]. Nói ngắn gọn, các nghiên cứu và khảo sát đều không ủng hộ hoặc tìm thấy rất ít bằng chứng cho thấy sự tồn tại của “bong bóng lọc”.</p><p> 2. Các “thuật toán” của chúng ta.</p><p> Tuy nhiên, những kết quả trên không phủ nhận và cũng không thể loại trừ sự phân cực về quan điểm trên môi trường mạng xã hội. Và phần lớn lỗi lầm nằm ở những đặc điểm cố hữu của chúng ta, thay vì ở các nền tảng.</p><p> Về cơ bản, việc các nền tảng sử dụng tốt thuật toán cá nhân hóa đều thành công và trở thành các tập đoàn công nghệ hàng đầu, cho thấy rằng họ đã làm tốt việc thỏa mãn nhu cầu cố hữu của con người. Chính con người, từ đầu đã luôn có xu hướng muốn tiếp xúc với những điều mình thích, thoải mái, đồng ý và bị thu hút. Vì thế, các nền tảng đã làm tốt việc trên để thỏa mãn người dùng, cùng lúc tận dụng để quảng cáo hiệu quả hơn và kiếm rất nhiều tiền từ mô hình kinh doanh này. Nhưng như đã chỉ ra, kể cả khi việc cá nhân hóa trên các nền tảng trực tuyến là điều không thể tránh khỏi, sự đa dạng quan điểm vẫn bị ảnh hưởng ở mức không đáng kể - ít nhất là cho tới hiện tại.</p><p> Ngoài ra, các khung pháp lý và đạo đức kinh doanh vẫn sẽ phần nào giữ vai trò là người gác cổng thông tin trên mạng. Đầu năm 2018, Google công khai thừa nhận những hạn chế trong việc hiển thị đoạn trích ngắn (snippet) trong kết quả đầu tiên của trang tìm kiếm, và tuyên bố sẽ khắc phục vấn đề này [9]. Có lẽ các ông lớn đều đã bắt đầu dè chừng hơn và biết lắng nghe người dùng nhiều hơn kể từ sau vụ bê bối Cambridge Analytica đã khiến ông chủ Facebook phải ra điều trần trước Quốc hội Mỹ không lâu trước đó [10].</p><p> Và kể cả khi bong bóng lọc thực sự tồn tại, cách giải quyết nó cũng không quá phức tạp: hãy chọc vỡ bong bóng. Hãy tiếp xúc với những cá nhân, những suy nghĩ và quan điểm nằm ngoài chiếc bong bóng của bạn. Nếu “bong bóng lọc” không phải là vấn đề quá nan giải và lỗi cũng không thuộc về thuật toán của các nền tảng mạng xã hội; có lẽ chúng ta nên tự xem lại các thuật toán của chính mình. Với những thiên kiến nhận thức và các cơ chế tâm lý cố hữu, liệu chúng ta có thể tự mình thoát khỏi chiếc bong bóng kia không?</p><p> Chúng ta đều biết đến thiên kiến xác nhận (confirmation bias) [11], khuynh hướng khiến chúng ta nghiêng về những thông tin mình ủng hộ nhằm tái xác nhận quan điểm hoặc niềm tin từ trước của mình. Nó chính là thứ khiến quá trình cá nhân hóa tự chọn của chúng ta bị bóp méo và lệch lạc. Nó khiến chúng ta chỉ tập trung tìm kiếm và tương tác với những gì chúng ta cho là đúng.</p><p> Và chúng ta có lẽ cũng từng nghe qua hiệu ứng buồng vang (echo chamber effect) [12], hình ảnh ẩn dụ về sự khuếch đại hay củng cố niềm tin khi được tiếp xúc với những thứ đúng ý mình. Từ việc lựa chọn tiếp thu, chia sẻ những quan điểm một chiều và bác bỏ những thứ trái chiều, chính chúng ta đã tự tạo ra căn buồng cô lập mình khỏi những luồng quan điểm khác, để rồi trở nên thiển cận, bảo thủ và cố chấp lúc nào không hay.</p><p> Chúng ta cũng biết về phân cực nhóm (group polarization), nói về việc khi những người cùng tin vào nữ quyền cùng ngồi lại, họ sẽ tạo ra một nhóm nữ quyền cực đoan. Hay nói cách khác, những người có niềm tin giống nhau khi cùng hợp tác thường tạo ra một niềm tin có mức độ cực đoan hơn bất kỳ niềm tin của cá nhân nào phía trong đó. Nếu chưa biết, có thể đọc thêm ở bài viết trước đây của Monster Box [13].</p><p> Hãy nhìn vào chính mình, có thể đôi lúc vấn đề trước nay luôn nằm ở chính chúng ta. Chẳng hạn, niềm tin rằng thế giới số là một nơi không đáng tin chứa đầy những thông tin tạp nham, rất có thể cũng là minh chứng cho việc ta đang bị nhốt trong chiếc “bong bóng lọc” của chính mình. </p><p> 3. Không  cực đoan nghĩa là cực đoan.</p><p> Hiện tại, giới nghiên cứu vẫn đang liên tục tranh cãi xoay quanh mức độ ảnh hưởng của echo chamber (có quá mức như chúng ta đang lo lắng?), cũng như có những phương pháp tính toán và nhìn chung, mọi thứ vẫn triệt tiêu lẫn nhau và cùng đưa về trạng thái cân bằng. Cho đến nay, chưa hề có bất kỳ kết luận đủ vững chắc nào cho thấy lỗi hoàn toàn thuộc về phía những người làm chủ các nền tảng (vì nếu vậy ắt hẳn các nền tảng đã bị lợi dụng một cách cực kỳ mạnh mẽ hoặc tẩy chay cực kỳ mạnh mẽ).</p><p> Có lẽ sự sôi động trong quan điểm của các nhà nghiên cứu về vấn đề trên, cũng giống với sự sôi động trên các nền tảng mạng xã hội. Một mặt, quả thật chúng ta có thể cùng những thuật toán tìm đến điều mình quan tâm, mặt khác, những người khác cũng thế và sự mâu thuẫn giữa các phe phái thực chất có thể giúp điều chỉnh quan điểm chung trở về gần với trạng thái cân bằng [14].</p><p> Điều này cũng hoàn toàn hợp lý, vì sự cực đoan trong quá khứ, khi chưa có mặt internet, thậm chí đã đủ để hình thành nên những trường phái như Phát Xít, hay các phe nhóm phân biệt chủng tộc như KKK. Chúng ta không thiếu những ví dụ tương tự trong quá khứ, nhưng lại khó nghĩ ra được bất kỳ phe phái nào cực đoan tương tự trong bối cảnh hiện nay.</p><p> Hay nói cách khác, chính môi trường internet đã khiến những xu hướng cực đoan nhanh chóng lên đến đỉnh điểm, va chạm với nhau rồi tan vỡ?</p><p> Tôi thỉnh thoảng vẫn ghé thăm những “vùng đất tăm tối” trên Facebook, những hội nhóm đông đúc chuyên phát tán điều tiêu cực, chuyên bóc phốt, hay cực kỳ tôn sùng mê tín dị đoan, hay chống phá nhà nước… đôi lúc tôi cảm thấy bối rối, bất ngờ hay thậm chí sợ hãi, nhưng đôi lúc tôi chấp nhận rằng đó là sự thật hiển nhiên đang tồn tại trong cuộc sống, bất kể ta có biết đến chúng hay chọn cách bỏ qua hay không. Và vì họ cũng là người, như tất cả chúng ta, nên tôi vẫn thường nghĩ ngợi xem vì sao mọi thứ lại chuyển biến theo hướng cực đoan nhanh như thế, và đông như thế.</p><p> Nhưng dù sao đi chăng nữa, khi ta khao khát, yêu cầu và ám ảnh trong việc muốn mọi người phải trung hòa thay vì cực đoan, thực chất cũng là một kiểu cực đoan khác. Thậm chí kiểu này còn nguy hiểm hơn vì chúng tạo ra cảm giác như thể chúng ta đang ở vị trí cân bằng đáng mơ ước của vũ trụ, dù tất nhiên không phải thế. Sự cân bằng ở mức độ cá nhân, không hơn không kém là một sự vô nghĩa.</p><p> Vì vậy, hãy chấp nhận sự tồn tại của những ý kiến trái chiều đến mức lệch hẳn khỏi thế giới quan của ta. Nhưng đừng lo lắng, sẽ có chiều ngược lại giúp trung hòa môi trường chung và mọi thứ cuối cùng sẽ lại ổn thỏa thôi.</p><p> #MonsterBox<br> ___________</p><p> THE INTERNET AND SOCIAL MEDIA: EXCHANGES OF VIEWS OR EXTREMISM CATALYSTS?</p><p> We often refer to the emergence of the internet and social media as the very precursor that unveils suggestions and broadens our human perspectives. Facebook, accordingly, is reckoned on as what connects people around the world, while Google is to make sure humans get the results of [roughly] whatever they search for.</p><p> Is it?</p><p> After all, is Facebook’s reign only hooks up like-minded extremists, who together become more overwhelmingly extreme, insofar as Google's eminence is purely to amalgamate every conspiracy theorists’ absurd argument, since they can always ferret out likeminds on Google?</p><p> To puzzle over which, we hereby put forward the very question of whether we could actually effectively, blatantly debate on the internet - the common accessible space to everyone, or would thereby “gang up” and more extremely polarize ourselves?</p><p> 1. Every internet (and major platforms’) algorithm</p><p> In 2010, Eli Pariser, a technology- and media- activist, put forward the so-called "filter bubble" concept, which described how major platforms’ algorithms had “distorted” our worldly perceptions. He, accordingly, asserted that every news and information users had ferreted out on search engines and social networks had been automatically promoted from the data those had voluntarily provided (on these platforms). .</p><p> Given the former usage patterns, for example, keywords or virtual interactions, algorithms were to sift through which and return appropriate matches to users. In his TED speech [1], Eli cited the experiment wherein two friends had googled "Egypt” - thereafter, whilst one caught news on crises and riots, the other, however, saw travel ads pop up on the first result page.</p><p> Eli termed it "personalized content" - the discrepancy that gets users boxed in by the information and political views they endorse; howbeit woefully heedless of every other contradictory and divergent opinion. To put it simply, Eli was gnawed at the fact that automatic algorithms might trap us in every filter bubble.</p><p> This, thus, is all too similar to the situation, in which we feel as though we had been exposed to every multifarious view on Facebook until we, on spur of moment, surf a stranger's newsfeed - a whole new world, else some eccentric-should-not-have-existed underworlds. Bizarrely enough, forasmuch as such worlds are overcrowded, you are the only one heedless of which.</p><p> Is it, nonetheless, that straightforward?</p><p> To seek an answer to this, let us scrutinize how internet users searched for information. Given the 2019 Digital News Report findings on 75,000 people from 38 different countries [2], social networks were actually sources. Thus, it seems inevitable that this type of media is piece by piece ousting traditional TV, newspaper and radio; insomuch as Facebook has been the most visited platform, accounting for 36%, which, along with WhatsApp, Instagram and Twitter, has got 84% of their users reckoning on which as their actual news feed - a dreary figure - given Eli’s "filter bubble" concept.</p><p> Subsequently, the subsistence of content personalization algorithms - the critical mechanism to optimize the total time amount the users virtually interact - is irrefutable. It, thus, is especially pivotal to every ecommerce platform: Amazon's product-recommendation feature has perked up the average value per order by 50%, winning the Brobdingnagian back an “insurmountable” growth rate of 300% [3]. That said, not every personalization has been automatically carried out by algorithms.</p><p> Withal this pre-selected personalization, there exists another form -  self-selected personalization [4], which is voluntarily “offered” by users. This is, after all, critical with regards to information and reception. </p><p> Before the advent of social networks, we by clicking and skipping a link, opting for newspapers and pages, or switching TV channels, every decision was indeed ours. On social networks, we are also turning a blind eye to which, disliking loathsome things, and sharing everything we favor. As always. As social media emerges, however, given our autonomy, which goes hand in hand with the display algorithms of these platforms, ain’t we limitedly exposed to meagre sources?</p><p> No. </p><p> Having compared three groups of users, one on purpose, and some others not intentionally turning to social media to gather information, and the rest not at all using which, a study [5] suggested that those homing in on social media, notably those getting on which for purposes other than reading were accidentally exposed to more sources of information than those who did not.</p><p> With regard to search engines, googlers must have intentionally ferreted out information on the engine. Results, thence, must have been double-personalized: algorithms opts for results, and we are the one to decide whether to click on which. Given subsistence of algorithms, might filter bubbles emerge?</p><p> Still, no. Zeroing in the survey data [6] from British, American German and Spanish users, results purported that news-searching users (1) referred to, on average, more online sources, (2) tilted toward both left and right news, and (3) earned themselves “equitable knowledge” since they had turned to both left- and right-wing sources.</p><p> Recently, every other study also yielded similar results. For example, when comparing search results of different people; especially American Democrats and Republicans, the in-between difference was negligible[7]. In addition, compared to news readers (frequently visited specific news sites); social network users were exposed to woefully more diverse news sources [8]. To put it simply, neither studies nor surveys have found, if any, little evidence on the actual existence of "filter bubbles".</p><p> 2. “Our” algorithms</p><p> However, such a finding can neither refute nor overrule the very polarization on social media, given that the fault lies not in the platforms, but every inherent human trait of ours.</p><p> At the very core, that every platform feasting on personalization algorithms succeeds and ends up tech Brobdingnagian evidences that they have performed exceptionally well at satisfying our inherent human desires. As humans, we, since the beginning, have got ourselves exposed to whatever we savor, agree with, are laid down by and sucked into. Therefore, the platforms have done a good job of doing the above to satisfy users, at the same time take advantage to advertise more effectively and make a lot of money from this business model. But as has been pointed out, even when personalization does exist, the diversity of opinion is not significantly affected - at least for now.</p><p> In addition, the legal framework and business ethics will still act somewhat as the gatekeeper on the internet. In early 2018, Google publicly imposed restrictions on displaying snippets in the first results of a search page, claiming to fix which [9]. In all appearances, every Brobdingnagian has got pussyfoot since the Cambridge Analytica scandal that urged the Facebook CEO to testify before the US Congress not long before [10].</p><p> And even if a filter bubble does exist, the solution appears any less effortless: pop it - get in touch with every individual, and initiative other than your own bubbles. Should the "filter bubble" appear not that arduous, nor does the fault ever lie in the algorithm of social network platforms; we should thence zero in ours. With cognitive biases and inherent psychological mechanisms, can we get rid of which on our own?</p><p> We, after all, know confirmation bias all too well [11]. It is the tendency that gets us tilted toward the information we back to reconfirm our previous beliefs, distorting our own personalization of choice, and homing us in on finding and interacting with whatever we reckon as righteous.</p><p> Withal, we must have every so often heard of the echo-chamber effect [12] - the metaphor of amplifying or reinforcing confidence once exposed to those invigorating which. From heeding, sharing one-sided views and refuting the opposites, we ourselves have created a chamber to isolate ourselves from other perspectives, to unconsciously turn out myopic and ultraconservative.</p><p> We know group polarization as well - as feminists gang up, they, as a rule, form an extreme feminist group. In other words, people with similar beliefs, upon rounding up, often stoke more extreme flames than any of the individuals within which. (Further information can be found in a former Monster Box article [13]).</p><p> Look at ourselves, for the problem, every so often, lies right in us. For example, the belief that the digital world is an untrustworthy place overflooded with trash sticks out as the very evidence that we have been trapped in our own "filter bubble".</p><p> 3. To stay anti-extreme is to stay extreme</p><p> To date, there has stirred up every argument within the research community over how well the echo chamber influences (whether it is indeed obnoxious), and every computational method to keep things equitable. So far, there has rarely been any strong-enough argument pointing to the fault being entirely on either of which (were there one alone, the platforms must have been either extremely woefully exploited or as much boycotted).</p><p> In all likelihood, the researchers’ excitement on some scientific matters is similar to that of social media platforms. On the one hand, even though we can leverage the algorithms to ferret out our interests, so can others, insomuch as conflicts between factions is what actually adjusts the common view to equilibrium [14].</p><p> Which makes perfect sense, for the past extremism, prior to the advent of the internet, was even enough to breed Nazis and every other racist group, such as K-K-K. Insofar as we had every time-honored example of which, we could rarely point at groups of equal extremism in the present context.</p><p> In other simplified words, is it the internet that has caused extremisms to peak, collide and shatter?</p><p> Every so often, I “pay a visit” to the "dark corners" on Facebook - the gorgeous groups that either fan out negativity, or dramacate everything, stay either woefully superstitious, or anti-state. Bewildering, even horror-stricken as it might seem, I, however, accept which as the obvious prevailing truth, irregardless of whether we are heedful of which or not. Since they (those groups’ members) are our compatriots, what we should heed is why things have got extreme that fast, and that predominantly.</p><p> Anyway, that we crave, thirst, and obsess about neutralism quelling extremism is de facto nothing but extremism. Which is, nonetheless, cripplingly onerous since it catalyzes the feeling as though we were in the desirable equilibrium. The balance on the personal level, to all appearances, is purely a vanity.</p><p> Thence, let us concede to the prevalence of those so-fiercely-conflicting-opinions-that-go-far-beyond-our-worldview. Nonetheless, stay quiescent, for there will be every dissident of which to neutralize the common ground, and after all, everything will be fine.</p><p> <a class="_58cn" href="/hashtag/monsterbox?__eep__=6&amp;source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARBYKey_Rrw-8zCg5qdrZxSAlIsGNyY-XSzER05cxPlU1p1LXHHTawb8d3yHeFrr66qQnYwbiC7QJUCp5Qp6-tLDZi4QQXFlScCg7US_stBallR0GNDNVrZPBZuyipD10fE16lWJG5nelKwi8eJaV91fGuFSUcZhGWpI8HDq6xa_sMwgML5axAodLZt04HAmZgzbif1q_T1OfnfNpDmZytW4ioQsx8K6PgHJFb6v7uMygYOHoH865xsZ-qwwqcs3kWy5H1DoP-gneQuG_DkOvEZLhF3-vAZW7zSYBF6IXfJCUhaOPEzAYMA&amp;__tn__=%2ANK-R" data-ft="{&quot;type&quot;:104,&quot;tn&quot;:&quot;*N&quot;}"><span class="_5afx"><span aria-label="hashtag" class="_58cl _5afz">#</span><span class="_58cm">MonsterBox</span></span></a></p><p> - Artist: Sam.<br> - Trans: Heinous.</p>