<p>[English Below]</p><p> TỪ “GÁY SỚM” Ở BẦU CỬ MỸ, CHO ĐẾN KỸ THUẬT DÙNG MẪU TRONG THỐNG KÊ.</p><p> Tại sao các hãng truyền thông lớn tại Mỹ (như The Associated Press, CNN, NBC, PBS, Fox News…) có thể tuyên bố ứng cử viên chiến thắng tại từng bang một cách chắc chắn khi quá trình kiểm phiếu vẫn chưa hoàn tất?</p><p> Tất nhiên những tuyên bố này vẫn gắn mác “dự đoán” (projected) và kết quả chính thức cần phải chờ đến khi 100% số phiếu bầu được đếm và công nhận, cũng như giải quyết các vấn đề kiện tụng pháp lý (nếu có). Nhưng rốt cục, hiện tượng “gáy sớm” này dựa trên căn cứ nào, và vì sao những căn cứ ấy đủ vững chắc để các tờ báo không vướng vào những cáo buộc pháp lý?</p><p> Bạn có quyền không tin vào kết quả được đưa ra bởi truyền thông Mỹ vì nhiều lý do (và vui lòng đừng lặp lại chúng ở phía dưới phần bình luận, việc này khá vô nghĩa). Nhưng có lẽ cũng nên thử dành chút thời gian xem hệ thống ra quyết định của các tổ chức truyền thông, và liệu “thế lực” nào chống lưng để họ có thể tuyên bố một cách chắc chắn như vậy về kết quả bầu cử.</p><p> Hiện nay, mỗi hãng tin sẽ có một “ban quyết định” (decision desk) với đội ngũ chuyên gia cùng phương pháp phân tích dữ liệu và dự đoán kết quả bầu cử riêng. Về cơ bản, họ đều dựa trên dữ liệu khảo sát người dân Mỹ trước và trong ngày bầu cử, cùng với kết quả kiểm phiếu được cập nhật liên tục tại các điểm kiểm phiếu để dự báo khả năng dành chiến thắng của từng ứng viên tại mỗi bang. Và, không có gì đáng ngạc nhiên khi xác suất thống kê là công cụ nền tảng đứng sau những kịch bản dự đoán này.</p><p> Để hiểu rõ hơn, hãy đến với hệ thống khảo sát AP-VoteCast, được nghiên cứu và phát triển bởi tổ chức truyền thông The Associated Press (AP) và trung tâm nghiên cứu phi đảng phái NORC tại Đại học Chicago, Mỹ. Theo đó, AP VoteCast sẽ thực hiện kết hợp hai cấp độ phỏng vấn (cấp tiểu bang và cấp liên bang) đối với hai đối tượng cử tri đã đăng ký: (1) chọn ngẫu nhiên từ hồ sơ cử tri của tiểu bang và (2) chọn theo những tiêu chí cho trước [1].</p><p> Cụ thể, trong kỳ bầu cử năm nay AP VoteCast thực hiện 50 cuộc khảo sát ở từng tiểu bang với tổng cộng khoảng 30,000 người thuộc nhóm chọn ngẫu nhiên và 110,000 người trong nhóm chọn theo tiêu chí cho trước. Ở cấp độ liên bang, họ thực hiện 1 khảo sát có tính đại diện toàn quốc với khoảng 3,000 cử tri đã đăng ký được chọn ngẫu nhiên trên hệ thống khảo sát AmeriSpeak (nền tảng cung cấp mẫu khảo sát thống kê có khả năng đại diện cho 97% số hộ gia đình trên toàn nước Mỹ).</p><p> Theo AP, kết quả phân tích dữ liệu thu được từ các khảo sát sẽ cho biết xu hướng chính trị của từng tiểu bang (ủng hộ các ứng cử viên tổng thống của Đảng Cộng hòa hoặc Dân chủ) sẽ tiếp tục duy trì hay đảo ngược. Đồng thời, đội ngũ chuyên gia của AP cũng thường xuyên liên hệ với nhóm kiểm phiếu để xác nhận các thông tin mới nhất về những lá phiếu đã được đếm và số phiếu bầu còn lại. Từ đó, họ có thể tính số phiếu phổ thông các ứng cử viên đang nắm giữ và tuyên bố người sẽ chiến thắng tại từng bang khi (và chỉ khi) tỷ lệ phiếu bầu có sự chênh lệch và chắc chắn không thể “lật kèo” với số lượng phiếu còn lại.</p><p> Đó là cách mà truyền thông Mỹ tuyên bố người chiến thắng (“called the race”) trong cuộc bầu cử tổng thống. Tuy nhiên, đây không phải là trọng tâm chính bài viết này muốn đề cập đến. Điều chúng tôi muốn nói ở đây là bằng cách nào hãng AP (hay các tổ chức truyền thông khác) xác định được số lượng mẫu khảo sát họ cần phải thực hiện cho các nghiên cứu thăm dò ý kiến này? Làm thế nào ý kiến của 3000 người lại đủ sức đại diện cho ý kiến của hơn 300 triệu người dân Mỹ? Bởi nếu số lượng mẫu quá ít sẽ không đủ cơ sở đưa ra kết luận, dẫn tới kết quả dự đoán không chính xác và không đáng tin cậy. Ngược lại, sử dụng quá nhiều mẫu khảo sát sẽ lãng phí thời gian và chi phí vô nghĩa. Vậy thì,</p><p> 1. Bao nhiêu là đủ cho một mẫu thống kê?</p><p> Đầu tiên, cần phải nhắc lại một số khái niệm cơ bản mang tính chất thống kê chúng ta ít nhiều đã từng nghe qua, như quần thể thống kê (statistical population) là một tập hợp các cá thể có một hoặc một vài điểm chung (ví dụ như tổng số cử tri Mỹ tham gia bầu cử) và mẫu thống kê (statistical sample) chỉ một tập hợp các cá thể (individuals) hoặc đối tượng quan sát (observations) được thu thập hoặc lựa chọn từ một quần thể dựa trên một phương pháp xác định [2].</p><p> Thông thường số lượng cá thể trong quần thể (population size) là rất lớn, nên việc thực hiện khảo sát đối với toàn bộ quần thể trở nên không thực tế và không khả thi. Và nếu để biết được quan điểm của 100 triệu người, bạn phải đi hỏi 100 triệu người, vậy sự tồn tại của khoa học dường như vô nghĩa.</p><p> Do đó, với nền tảng lý thuyết xác suất và thống kê, người ta có thể lựa chọn ngẫu nhiên (hoặc không ngẫu nhiên) một số lượng xác định các cá thể trong quần thể ấy để phân tích và đưa ra những suy diễn thống kê cho cả quần thể tùy theo ý đồ. Tất nhiên những suy diễn này vẫn phải tuân theo các điều kiện giả định cho trước (background assumptions) để có ý nghĩa thống kê (statistical significance).</p><p> Thật ra, không có công thức tổng quát cho tất cả các trường hợp. Những con số định lượng như 1000, 10000 hay 10 không hề cho thấy nó đáng tin hay không đáng tin. Việc xác định cỡ mẫu phụ thuộc chủ yếu vào mục tiêu nghiên cứu. Ví dụ, nghiên cứu thăm dò ý kiến như phần đầu của bài viết, nếu muốn đánh giá số lượng mẫu có đủ hay không, câu hỏi đầu tiên phải đặt ra là kết quả khảo sát có độ chính xác đến đâu? Hay theo ngôn ngữ thống kê, biên độ sai số (margin of error) và mức độ tin cậy (confidence level) bạn chọn là bao nhiêu? Khi đã xác định được hai điều kiện này cùng với một số công thức thống kê, ta có thể dễ dàng tính được kích thước mẫu cần thiết cho khảo sát [3], [4].</p><p> Ví dụ, giả sử AP VoteCast đã thực hiện một cuộc khảo sát ngẫu nhiên với 2000 người tại bang Washington với biên độ sai số là ±2.3% và mức độ tin cậy 95%. Kết quả khảo sát cho thấy tỷ lệ ủng hộ Joe Biden đạt 58.8%. Ta có thể diễn dịch kết quả này như sau: “Nếu lặp lại khảo sát này một cách độc lập 100 lần, ta có thể kỳ vọng có 95 lần tỷ lệ ủng hộ Joe Biden nằm trong khoảng 58.8% ± 2.3%”.</p><p> Theo lý thuyết, với biên độ sai số và mức độ tin cậy như trên, chỉ cần thực hiện khảo sát ngẫu nhiên và không bị thiên vị chọn mẫu (selection bias) với ít nhất 1816 người, kết quả thu được từ tập mẫu có thể đại diện cho kết quả của toàn bang. Do đó, theo lý thuyết AP VoteCast hoàn toàn có thể dự đoán tỷ lệ ủng hộ Joe Biden tại Washington sau khi khảo sát 2000 người tại bang này. Tuy nhiên, kết quả này vẫn chỉ có ý nghĩa về mặt thống kê mà thôi. Vậy,</p><p> 2. Tính đại diện (representativeness) của mẫu thống kê là gì?</p><p> Trong nghiên cứu, việc chọn đúng mẫu thống kê cũng quan trọng không kém việc xác định kích thước mẫu. Khi thực hiện nghiên cứu về một quần thể nào đó, người ta thường cố gắng chọn những mẫu khảo sát có tính đại diện, mang các đặc điểm, tính chất của quần thể. Nhiều người cho rằng nếu tập hợp mẫu không có tính đại diện thì kết quả nghiên cứu không có ý nghĩa khoa học. Nhưng sự thật có như vậy không?</p><p> Có hai yếu tố chính khi đánh giá tính đại diện là (1) sự tương đồng (similarity) và (2) sự ngẫu nhiên (randomness). Nhiều người thường nghĩ rằng để mẫu thống kê có tính đại diện, cần phải lựa chọn ngẫu nhiên các đối tượng trong mẫu sao cho có sự tương đồng với quần thể được nghiên cứu, ví dụ:</p><p> - Tương đồng về đặc điểm thống kê: Nếu trong tổng số những người sử dụng điện thoại di động có 88% thuê bao trả trước, 12% trả sau và 45% sử dụng 3G&amp;4G thì khi chọn tập mẫu cũng phải có phân bố tương tự.</p><p> - Tương đồng về kết quả: Nếu trong quần thể người có 38% dân số bị tiểu đường thì mẫu nghiên cứu cũng phải đạt tỷ lệ 38%, nhiều hơn hay ít hơn đều không mang tính đại diện.</p><p> - Tương đồng về phân bố địa lý: Nếu dân số TP. HCM và Hà Nội chiếm tỉ lệ lần lượt là 3.5% và 1.5% dân số cả nước thì mẫu nghiên cứu cũng phải có tỉ lệ như vậy.</p><p> Tuy nhiên những cách hiểu trên đều là những ngộ nhận về việc lựa chọn mẫu nghiên cứu. Không phải tất cả các nghiên cứu khoa học đều cần mẫu có tính đại diện. Việc chọn mẫu có tính đại diện như trên chỉ (có thể) phù hợp với các khảo sát, điều tra xã hội khi hoạch định chính sách [5].</p><p> Còn đối với các nghiên cứu khoa học, việc chọn mẫu theo tính đại diện trong nhiều hợp sẽ là vô nghĩa hoặc đôi khi phản tác dụng. Ví dụ, với nghiên cứu về tỷ lệ mắc các bệnh tim mạch trong quần thể thì việc lựa chọn mẫu theo cấu trúc độ tuổi của quần thể sẽ ảnh hưởng đến độ chính xác của nghiên cứu. Dù tỉ lệ mắc các bệnh tim mạch ở nhóm tuổi 60-80 cao hơn nhóm tuổi 20-40, nhưng do số lượng người tham gia khảo sát thuộc nhóm 60-80 ít hơn nhóm 20-40 nên sẽ làm giảm tỷ lệ mắc các bệnh tim mạch trong tập mẫu (so với tỷ lệ thực tế).</p><p> Những ngộ nhận về tính đại diện xảy ra phổ biến trong các nghiên cứu khoa học vì nó dễ áp dụng khi thiết kế thí nghiệm. Nếu chỉ dựa vào tính đại diện, nhiều khả năng sẽ dẫn đến các kết luận sai lầm. Trên thực tế, thứ gì đó có tính đại diện hơn không (thực sự) khiến nó có nhiều khả năng xảy ra hơn.</p><p> Thế nên, điều chúng ta cần quan tâm khi thiết kế mẫu nghiên cứu là độ lặp lại (repeatability) và tái lập (reproducibility) nghiên cứu. Một nghiên cứu có khả năng lặp lại và tái lập cho thấy kết quả có ý nghĩa khoa học (scientific significance) chứ không chỉ dừng lại ở ý nghĩa thống kê. Và tính đại diện khoa học (scientific representativeness) là khi có thể lặp lại và tái lập kết quả nghiên cứu ở bất kì quần thể nào, không phụ thuộc vào thời gian và không gian của thí nghiệm nghiên cứu.</p><p> Tóm lại, nên chọn mẫu nghiên cứu như thế nào?</p><p> 3. Kỹ thuật chọn mẫu thống kê</p><p> Tùy theo định hướng nghiên cứu mà chúng ta có nhiều cách để xác định mẫu dữ liệu và kích thước mẫu. Phương pháp lấy mẫu dữ liệu cũng vậy. Tuy nhiên, có thể chia làm hai loại chính: (1) lấy mẫu ngẫu nhiên (probability sampling) và (2) lấy mẫu phi ngẫu nhiên (non-probability sampling) [6].</p><p> Hiểu một cách đơn giản, phương pháp lấy mẫu ngẫu nhiên sử dụng các hàm lựa chọn ngẫu nhiên (random sampling), do đó xác suất từng cá thể được chọn vào tập mẫu là ngang nhau và có thể tính được. Ví dụ, Chọn ngẫu nhiên 1 viên bi trong n viên bi, thì xác suất mỗi viên bi được chọn đều là 1/n. Trong khi đó, lấy mẫu phi ngẫu nhiên chỉ dựa vào các giả định chủ quan ban đầu của người nghiên cứu, do đó không thể tính chính xác khả năng mỗi cá thể được chọn.</p><p> Để so sánh, ưu điểm chính của cách lấy mẫu phi ngẫu nhiên là sự hiệu quả về chi phí và thời gian, thường được sử dụng khi không thể tiến hành lấy mẫu ngẫu nhiên (ví dụ: khi bạn khảo sát một quần thể rất nhỏ). Nhưng nhược điểm là không thể tính toán khoảng tin cậy và biên sai số. Bên cạnh đó, lấy mẫu phi ngẫu nhiên sẽ đặt ra giới hạn về lượng thông tin mà một mẫu có thể cung cấp về quần thể tương ứng, nên khó có thể ngoại suy từ mẫu ra quần thể. Hạn chế này còn được gọi là thiên vị lựa chọn (selection bias), mà nếu không chú ý đến khi thực hiện nghiên cứu, nhiều khả năng sẽ dẫn đến các kết luận sai lệch.</p><p> Trong bài kỳ trước [7], chúng tôi có nhắc đến “thiên vị xuất bản (publication bias) – việc tạo ra sự sai lệch trong nhận thức của cộng đồng bằng cách không công bố những kết quả bất lợi (thường là tiêu cực) hoặc kết quả đi ngược lại với định kiến của người thử nghiệm, lợi ích của nhà tài trợ hoặc kỳ vọng của cộng đồng. “Thiên vị lựa chọn” cũng có liên quan chặt chẽ đến vấn đề này.</p><p> Chẳng hạn, việc khảo sát ở bang California về cảm tình của người dân trong bang này với các nghị sĩ của phe Cộng hòa, rồi dùng kết quả đó đại diện cho toàn nước Mỹ, sẽ là một trò đùa hay thay vì một khảo sát có ý nghĩa. </p><p> Nhưng thiên vị lựa chọn còn thường xuyên xảy ra bất kể nhà nghiên cứu không cố ý, chẳng hạn, kết quả thu được từ tình nguyện viên đôi khi cũng gây ra sai lệch. Đây được gọi là “thiên lệch tình nguyện viên” (volunteer bias) [8]. Vì phần lớn các tình nguyện viên tham gia các nghiên cứu, khảo sát thường có độ tuổi, mức thu nhập, nhận thức xã hội đặc thù và không thể đại diện cho quần thể lớn hơn. Đó là chưa kể đến chênh lệch giới tính (nữ nhiều hơn nam) và thái độ của họ trước khi tham gia các nghiên cứu, thí nghiệm, khảo sát. Hoặc nếu khảo sát ngẫu nhiên thông qua điện thoại, có thể kết quả cũng bị lệch vì một số người có thói quen không nhấc máy hoặc từ chối, số khác lại sốt sắng, thích thú.</p><p> Mặc dù “thiên vị lựa chọn” thường xảy ra do xu hướng chung của con người là chú ý nhiều hơn đến những khả năng có thể xác nhận quan điểm đã có từ trước của mình (thành kiến xác nhận [9]). Nhưng trong một số trường hợp, sự bóp méo được tạo ra bởi các thí nghiệm ‘cố tình’ được thiết kế để tìm kiếm bằng chứng xác nhận thay vì cố gắng bác bỏ một giả thuyết.</p><p> Trong “giới hàn lâm”, có một cụm từ luôn gây ám ảnh với các nhà nghiên cứu khoa học là “Publish or Perish” (tạm dịch: Xuất bản hay là chết) để nói đến áp lực phải chạy đua xuất bản những nghiên cứu khoa học. Đã từ rất lâu, công bố khoa học được xem như là sinh mệnh và cũng là thước đo để đánh giá năng lực các nhà nghiên cứu. Một nhóm nghiên cứu có được tài trợ (funding) tiếp hay không đều phụ thuộc vào các công bố khoa học của họ.</p><p> Khi quỹ tài trợ có hạn mà số nhóm nghiên cứu ngày càng tăng thì vấn đề cạnh tranh là không tránh khỏi. Áp lực công bố sinh ra bởi sự cạnh tranh này dường như thúc đẩy nền khoa học phát triển, nhưng đôi khi cũng khiến những người thực hiện nghiên cứu “đánh mất chính mình”, từ bỏ liêm chính khoa học để “tạo ra” các kết quả có lợi cho việc xuất bản. Và rất khó để công chúng có thể phát hiện ra sai phạm trong những nghiên cứu này.</p><p> Thế nên, ngoài việc hi vọng những người làm nghiên cứu sẽ luôn tỉnh táo trước những áp lực xuất bản, thì chúng ta – những người đọc phổ thông phải luôn giữ trong mình sự hoài nghi về những thông tin nhận được. </p><p> “Because if a claim seems too good—or too bad—to be true, it probably is”.</p><p> #MonsterBox<br> ___________</p><p> HOW PEOPLE ARE MAKING “EARLY CALLS” IN 2020 U.S. PRESIDENTIAL ELECTION, AND THE SAMPLING TECHNIQUES IN STATISTICS.</p><p> Why can major U.S. cable news networks (such as The Associated Press, CNN, NBC, PBS, Fox News…) make definite claims about which candidate to win each of the states while the vote counting process is still not entirely completed?</p><p> Naturally, these results are still labeled as “projected” (projected) and official results can only be announced when 100% of the votes are counted and recognized, as well as all legal disputes (if any) are resolved. If so, then what is the basis for these “early calls”, and how is that basis reliable enough for the said news network to not get in legal troubles?</p><p> You have the right to not believe in the election results provided by the U.S. news media for many different reasons (and, please, do us a favor by not restating those arguments in the comment section. We do get what you are saying, but those do not help with the purpose of this article). But maybe we really should, even though just for a bit, look into the system of decision making used by the cable news channels, to see whether there is any “power” that is backing them so they can make such calls on the election results with pretty much absolute certainty .</p><p> Nowadays, each cable news channel would have for themselves a “decision desks” consisting of a strong team of experts, who will employ the channel’s exclusive methods for data analysis and result projection. To put it simply, all of these channels used the data gained from the election polls conducted before and during the election day, along with the counting results which is constantly updated at vote counting venues to forecast the win probability of each candidate at each of the states. And, not surprisingly, probability and statistics is the foundational principle behind these projective scenarios.</p><p> To understand it better, we should first take a look at AP-VoteCast, a system for public opinion survey researched and developed by the media agency The Associated Press (AP) in collaboration with the non-partisan research institute NORC at the University of Chicago, U.S.A. In the course of which, AP VoteCast will conduct surveys on two different level (state-level and national level) on two samples of registered voters: (1) one is formed by randomly selecting voters from the individual states’ voter registries, (2) the other by selecting by predetermined criteria [1].</p><p> Specifically, in this year’s election, AP VoteCast conducted 50 surveys at each of the states, with 30,000 participants in the randomly selected sample and 110,000 in the selected-by-criteria group. On national level, they conducted one nationally-representative survey on 3,000 voters randomly selected on the survey system AmeriSpeak (a survey-sample-provider platform that is capable of representing 97% households all across the U.S.A).</p><p> According to AP, the data analysis results gained from these surveys will reveal the political tendencies of each individual state (whether they support the Democratic or Republican presidential candidate), and whether these tendencies shall be retained or reversed. What’s more, the team of experts from AP would also frequently contact the vote counters to verify the latest intel on counted votes and the remaining uncounted votes. Based on that, they can make an estimation on the amount of popular votes held by each of the candidates and announce the candidate to win an individual state when (and only when) there is a significant lead in terms of vote for either of candidate, that is sufficient to ensure than an a comeback is impossible even with the rest of the votes.</p><p> This is how the U.S. media managed to call the presidential race. However, this is not the main point of this article. What we wish to discuss here is what method did AP (or any other media agency) use to determine the exact sample size they needed for these public opinion surveys? By what process can the opinions of 3000 people become able to represent for that of the entire 300 million American people? An overly small sample would not provide sufficient basis for a conclusion, and thus would lead to an incorrect and reliable prediction; while an overly large sample, on the other hand, would waste time and resources. Therefore, the question we need to answer is</p><p> 1. How large should a sample be?</p><p> First, we need to revise some fundamental statistical concepts that we have more or less heard of, such as ‘statistical population’ – a set of individuals with one or multiple things in common (for example, all the American voters in this election) and ‘statistical sample’ – a set of individuals or observations collected or selected from a population by using certain sampling method [2].</p><p> In most cases, the population size would be so large, that conducting a survey on the entire population would be impossible and infeasible. And if it’s true that in order to know the opinions of 100 million people you must ask each one of them, then the existence of science would seem rather pointless.</p><p> Therefore, by applying the theoretical basis of probability and statistics, people can randomly (or not so randomly) select a certain number of individuals within the population to serve as a sample for their statistical analysis, through which they can make statistical inference applicable for the entire population, depending on their intention. Still, these inferences must satisfy the predetermined background assumptions in order to achieve statistical significance.</p><p> In reality, there is no all-powerful formula that is applied for all cases. Quantitative numbers like 1000, 10000 or 10 by themselves do not reflect the sample’s reliability. The determination of sample sizes actually depends mainly on the objectives of the studies. For example, for a public opinion survey like the one we mentioned earlier in the article, in order to determine whether a sample is large enough, the first question we should ask is how accurate we wish the survey results to be? Or, in the words of statistics, what is the margin of error confidence level you wish to select? By determining these two conditions, along with the use of a few statistical formulae, we can easily calculate the sample size necessary for the survey [3], [4].</p><p> For example, if AP VoteCast conducted a survey with a randomly selected sample of 2000 people from the State of Washington with a margin of error of ±2.3% and a confidence level of 95%, the survey results would show that the ratio of Joe Biden’s supporters in the population was 58.8%. We can interpret this result as follows: “If we repeat this same survey independently for 100 times, we can expect that in 95 out of 100 times the survey is conducted, the ratio of Joe Biden’s supporters in the population would be somewhere within the range of 58.8% ± 2.3%”.</p><p> In theory, with the said margin of error and confidence level, just by conducting random surveys with selection bias entirely eliminated on a sample of at least 1816 people, the results gained from the sample would be representative for the entire State’s population. Thus, theoretically, AP VoteCast absolutely can predict the proportion of voters supporting Joe Biden in the State of Washington of surveying 2000 people from this state. However, this result only has significance in terms of statistics. Then, the next issue we have to discuss would be</p><p> 2. What is a statistical sample’s ‘representativeness’?</p><p> In scientific studies, selecting the right sample is just as important as selecting the right sample size. When conducting studies on a certain population, people would most oftentimes try to select a sample that is ‘representative’, meaning that they are vested with the key characteristics and attributes of the population. Many believe that if the sample does not ensure representativeness then the study results would not be scientifically significant. But the question is, is that necessary true?</p><p> There are 2 main criteria used for assessing a sample’s representativeness, namely (1) the similarity and (2) the randomness. People most oftenly believe that in order for a statistical sample to gain representativeness, the individuals within the sample must be randomly selected so that the sample have a certain level of similarity with the studied population, for example:</p><p> - Similarity in terms of statistical characteristics: If among the entire population of mobile phone users 88% use prepaid service, 12% use postpaid service and 45% use 3G&amp;4G service then the selected sample must have similar distribution.</p><p> - Similarity in terms of results: If among the population 38% suffer from diabetes, then the study must also show a result of 38%, any less or more would mean the sample is not representative.</p><p> - Similarity in terms of geographical distribution: If the population of HCMC and Hanoi account for respectively 3.5% and 1.5% of Vietnam's population then the sample must portray the same distribution.</p><p> However, the said interpretations are but misconceptions about how sampling is done in studies. Not all scientific studies require a representative sample. The selection of a representative sample as mentioned above would only (probably) work for social surveys conducted to facilitate the policy making [5].</p><p> As for scientific research, sampling by representativeness would in many cases be either pointless or even counter-productive. For example, in a study on the rate of contracting cardiovascular diseases in a population, the selection of samples in accordance with the age group distribution of the population will hamper the accuracy of the studies. Even though the rate of contracting cardiovascular diseases among the age group of 60-80 might be higher than that of the age group 20-40, but as less participants are from the age group 60-80 than the group 20-40, the rate of cardiovascular disease within the sample might actually be underestimated (in comparison to the actual rate).</p><p> The misconceptions on representativeness happen most commonly in scientific research, because it is actually easy to apply in designing the experiment. If the sample is selected entirely based on representativeness, false conclusions more than likely will occur. In reality, something being more representative does not (necessarily) mean it is more likely to happen.</p><p> Therefore, what we really need to care about when designing a research sample is the repeatability and reproducibility of the research itself. If a research is highly repeatable and reproducible, it means that it is scientific significant and not just statistically significant. And scientific representativeness means that the results are repeatable and reproducible on any population, without being dependent on the time and place where the research is conducted.</p><p> So, in the end, how exactly should we select our research sample?</p><p> 3. Sampling techniques</p><p> Depending on the research orientation, various different methods can be employed to determine the type and size of sample needed. The same is true for sampling methods. However, the sampling methods can generally be categorized into 2 types: (1) probability sampling and (2) non-probability sampling [6].</p><p> To put it simply, probability sampling uses random sampling functions, so that the probability for each individual in the population to be selected for the sample is identical and entirely quantifiable. For example, if you randomly select 1 marbles among n marbles then the probability for any of the individual marbles to be chosen is still 1/n. Meanwhile, non-probability sampling is done based on the researcher’s subjective assumptions, and thus the individuals’ probability of being selected cannot be precisely quantified.</p><p> For comparison, the most important advantages of non-probability sampling are cost- and time-efficiencies, and this method is often employed where probability sampling is infeasible (for example: when the observed population is very small in size). However, the method also has its downside, which is the impossibility of quantifying confidence interval and margin of error. What’s more, non-probability sampling also limits the amount of information that a sample can provide about its respective population, and thus causes difficulties for extrapolations from sample to population. This disadvantage is often known as the selection bias, which, if left unnoticed during the course of the research, will very much likely lead to false conclusions.</p><p> In our previous articles [7], we had referred to the concept of “publication bias” – which is the practice of inducing misconception in the public mind  by not announcing results that are unfavorable (which are most often time negative) or contrary to the researchers’ preexisting bias, the sponsor’s interest or the public’s expectation. “Choice-supportive bias” are also closely related to this issue.</p><p> For example, if someone conducts a survey on the people of California’s support for Republican senators, and then uses results gained from that survey to represent the entire Americans’ opinion, it would be more of a satire than an actually useful survey. </p><p> However, choice-support bias more than often would still happen anyway, with or without the researcher being intentional about it; for example, the unprocessed results gained from participants can also sometimes be misleading. This phenomenon is often known as the ‘volunteer bias’ [8]. This is due to the fact that most oftentimes the volunteers to participate in researches and surveys do have their specific demographics in terms of age, income and social consciousness and thus cannot represent a larger population. This is even before we take into account the gender imbalance (for example, when there are more female participants than males) and their attitude before participating in any research, experiments or survey. Or even if a random survey is conducted via phone calls, the results might still be biased because people with certain demographics would have the tendency to not take the call or to refuse to take the survey, while some other would be more willing and interested in participating.</p><p> It’s indeed true that “choice-supportive bias” is prone to happen due to the universal tendency of humankinds to pay more attention to the possibilities that can serve to confirm their preexisting bias (the ‘confirmation bias’ [9]). But in some cases, distortion can be generated in experiments ‘intentionally’ designed to find results that confirm instead of reject a hypothesis.</p><p> Among the academic world, there is this certain maxim that the scientists have always been obsessed over: “Publish or Perish” (which means if your research is not published, it’s as good as abolished); which means to talk about the crazy pressure on the researchers to rush for the publication of their scientific researches. Since ever so long ago, scientific publications have been considered the lifeline as well as the metric for capability of researchers. Whether or not a research team receives funding depends entirely on their scientific publications.</p><p> As the funding pool is limited and the number of research teams continues to grow, competition is simply unavoidable. While this pressure for publication stemming from competition seems to promote development of science, it sometimes also causes researchers to “sell soul to the devil”, as they sack their scientific integrity to ‘fabricate’ results that help with publication. And this is even more tempting as it is virtually impossible for the public to detect the misconducts committed in these researches.</p><p> Therefore, apart from just hoping that researchers can keep their head cool before the pressure for publication, it would be best for us – the common readers – to retain a certain level of skepticism toward any information that is served to us. </p><p> “Because if a claim seems too good—or too bad—to be true, it probably is”.<br> <a class="_58cn" href="/hashtag/monsterbox?__eep__=6&amp;source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARDZJqI-eUC61VDmjb_1XoAt5hX5e-Tzy8YV0sWMV8yrK2QBNTwORfAlDohrTfIJ9Cv6TfUDYZ95NwfVY8vOnkZIjJx5jL124uTL5GoUYPyH8SOVt94zrW9vcAQucOLns5MeryTsSG9441MUxTfq8jc-akYoB0soE7sKCBrM3Xaqbp6VtvmNuPaoYCCa1eQp4cZEEu5wRJeI68RSMflc03RUh1pVng_JASt9EiiArQb6Wn6OaxjTqqfroUEj5foQ_UINQsH0pR_NG1GVTxh-BSv2vXQKV6f8MEBCoiSD8HPbS5E1CcFDrX0&amp;__tn__=%2ANK-R" data-ft="{&quot;type&quot;:104,&quot;tn&quot;:&quot;*N&quot;}"><span class="_5afx"><span aria-label="hashtag" class="_58cl _5afz">#</span><span class="_58cm">MonsterBox</span></span></a></p><p> - Artist: Sam.<br> - Trans: Useless Ponko.</p>